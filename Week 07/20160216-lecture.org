#+AUTHOR: Ryan Sharif
#+TITLE: Prolog Theory continued
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{mathtools}
#+LATEX_HEADER: \usepackage{tikz}
#+LaTeX_HEADER: \usepackage[T1]{fontenc}
#+LaTeX_HEADER: \usepackage{mathpazo}
#+LaTeX_HEADER: \linespread{1.05}
#+LaTeX_HEADER: \usepackage[scaled]{helvet}
#+LaTeX_HEADER: \usepackage{courier}
#+LATEX_HEADER: \usepackage{listings}
#+LATEX_HEADER: \usetikzlibrary{positioning,calc}
#+LaTeX_CLASS_OPTIONS: [letter,twoside,twocolumn]
#+OPTIONS: toc:nil

* Brief exploration in Artificial Intelligence
Last week,  we covered  propositional and  predicate logic.  There are
ways  that we  can simplify  formulas. For  example, we  could replace
$\exists x(q(x))$  can be rewritten  as $\neg \forall x  (\neg q(x))$.
This  way of  rewriting traditional  logic is  called /clausal  form/.
Your problem  is a set of  clauses, noting that these  clauses are all
true: $A_1 \wedge A_2 \wedge ... \wedge A_n$.

Horn clauses  give us the restriction  where $n \leq 1$,  i.e., he has
gotten rid of  the `or' sign (a subset of  first-order logic). We have
three possibilities, where $n = 1, m \geq 1$, $n = 1, m = 0$, and $n =
0$.

If we look at the inference  steps in Prolog, in general a computation
can be  thought of  as saying, we  have a bunch  of Goals:  $G_1, G_2,
.... G_p$. Prolog looks at our leftmost goal.

** Implementing a Prolog interpreter via a stack
As  your goals  are added,  you push  subgoals to  the stack.  You pop
subgoals when  they are  succeeded by  facts. But  there is  more than
success for  pushing and popping. Success  in a match can  also push a
choice point. Thus, failure pops the  stack to the newest choice point
and resumes from there. Success, then, omits newests goals, but cannot
pop all useful  items, keeping new bindings. Failure does  most of the
real popping. Since our stack will keep growing on success, the Prolog
interpreter will garbage collect the stack.

* Storage management
** Lifetime of storage
Professor Eggert had a problem with the following section of code:

#+BEGIN_SRC c
  if () {
   } else if (n <= 8) {
     Lisp_Object a[8];
     a = ...;
     f(a);
   } else if (n < 1024) {
    List_Object *b = alloca(n * sizeof(*b));
   } else (n >= 1024) {
    List_Object *c = malloc(n * sizeof(*c));
    ...;
    free(c);
   }

#+END_SRC

The lifetime of an ~alloca~ function is that of the calling function.
The implementation given above had the allocation of arrays as most
common. Professor Eggert says that the program would dump core on
occassion. The program would only free the memory created by ~alloca~
when the function ended. This bug stemmed from the lifetime of storage.

** Possible lifetimes

*** Static 
The simplest lifetime of a C program is static:
#+BEGIN_SRC c
int x;
#+END_SRC
The storage lasts for the entirety of the runtime.
Static allocation means we cannot have useful recursion.

*** Static and Stack dynamic 
C is an example of a program that allows us to have static
storage along with a dynamically allocated stack. The size of 
the activation records are of _fixed size_.

#+BEGIN_SRC c
  int f(int a, int b) {
    int v = [];
    if ( x == 5) {
      int y[10];
    } else {
      int z[5];
    }
  }
#+END_SRC

Since we know from the function above, that y and z can both fit in an
array that is  maximally the size of z, the  compiler can generate one
instruction for allocation and  one instruction for deallocation. This
approach is popular but it too has its own downsides.

The lifetime of an array is the block that it occurs in, the lifetime,
rememeber, of ~alloca~  is that of the calling  function. What happens
when  $n <  0$?   If we  are  lucky,  the program  crashes.  If ~n  ==
INT_MAX~, then the program crashes. The distance between g's frame and
f's frame is variable. A dynamically sized array also requires a frame
pointer, in other words more than just a stack pointer.

Another problem occurs in nested functions. Consider the following code:
#+BEGIN_SRC c
  int f (int x) {
    int g(int y) {
      return x + y;
    }
  }
#+END_SRC

It's easy to get the value inside g's stack, but there is a problem with
obtaining the value of x, a value stored in f's stack. We can think of
the stack as a linked list of activation records. This linked list is
called the dynamic chain, where the `next' field points to the caller.
For getting the variables we're looking for, we need to consider the
/static chain/. The static chain is shorter than the dynamic chain
and can hop before the dynamic chain. The number of hops in the static
chain is equal to the number of nesting in the source code.

The way to implement currying in C, is that we need to reclaim an
activation record only if no one is pointing at it. Here we say
reclaim for the garbage collector. Thus, Activation Records live
on the heap. And we know that the heap persists as long as it is
referenced, whereas the stack exists until a function exits.

*** Register storage
/Leaf functions/ are those that don't call any other functions. Leaf
variables allow us to avoid the stack all together. 

*** Persistent storage
There are variables that persist indefinitely. These variables have
the same variables when your program stopped last time.

*** Lifetime vs. Scope
If you are looking at a function, and if it declares a variable and
you use it. A variable's scope is a subset of its lifetime.
